{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1_JftWJCKBnLZSkwiN8Oss5Sv1LFVAaZ5",
      "authorship_tag": "ABX9TyNM3tYgLaaV3eZuL9ApDtBQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rockuzzer/test/blob/main/Capstone_CAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DOMAIN: Automotive Surveillance.\n",
        "\n",
        "CONTEXT:\n",
        "Computer vision can be used to automate supervision and generate action appropriate action trigger if the event is\n",
        "predicted from the image of interest. For example a car moving on the road can be easily identified by a camera as make of\n",
        "the car, type, colour, number plates etc.\n",
        "\n",
        "DATA DESCRIPTION:\n",
        "The Cars dataset contains 16,185 images of 196 classes of cars. The data is split into 8,144 training images and 8,041 testing\n",
        "images, where each class has been split roughly in a 50-50 split. Classes are typically at the level of Make, Model, Year, e.g. 2012 Tesla Model S or 2012 BMW M3 coupe.\n",
        "Data description:\n",
        "\n",
        "*   Test Images: Consists of real images of cars as per the make and year of the car.\n",
        "*   Train Images: Consists of real images of cars as per the make and year of the car.\n",
        "*   Train Annotation: Consists of bounding box region for training images.\n",
        "*   Test Annotation: Consists of bounding box region for testing images.\n",
        "\n",
        "Dataset has been attached along with this project. Please use the same for this capstone project.\n",
        "Original link to the dataset for your reference only: https://www.kaggle.com/jutrera/stanford-car-dataset-by-classes-folder [ for your\n",
        "reference only ]\n",
        "\n",
        "Reference: 3D Object Representations for Fine-Grained Categorisation, Jonathan Krause, Michael Stark, Jia Deng, Li Fei-Fei 4th IEEE\n",
        "Workshop on 3D Representation and Recognition, at ICCV 2013 (3dRR-13). Sydney, Australia. Dec. 8, 2013."
      ],
      "metadata": {
        "id": "wUh98VvwFa1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0PQWcvuHczF",
        "outputId": "cf07c282-0226-4dc7-b45e-91c887e13411"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from zipfile import ZipFile\n",
        "import glob\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mp"
      ],
      "metadata": {
        "id": "E91I9sKiHnAe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Import the data."
      ],
      "metadata": {
        "id": "RZOp79JAG1sA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqMIOuCLFM1Y"
      },
      "outputs": [],
      "source": [
        "# Step 1: Import the data\n",
        "\n",
        "# Path to the zip files\n",
        "annotations = \"/content/drive/MyDrive/AIML/Capstone Project/Annotations.zip\"\n",
        "car_images = \"/content/drive/MyDrive/AIML/Capstone Project/CarImages.zip\"\n",
        "\n",
        "# Directory to extract the data\n",
        "extracted_dir = \"/content/drive/MyDrive/AIML/Capstone Project/extracted_data\"\n",
        "\n",
        "# Function to extract zip files\n",
        "def extract_zip(zip_path, extract_to):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "# Extract the annotations\n",
        "extract_zip(annotations, extracted_dir)\n",
        "\n",
        "# Extract the car images\n",
        "extract_zip(car_images, extracted_dir)\n",
        "\n",
        "# List the contents of the extracted directory\n",
        "extracted_contents = os.listdir(extracted_dir)\n",
        "print(\"Contents of the extracted directory:\", extracted_contents)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Map training and testing images to its classes."
      ],
      "metadata": {
        "id": "AU7_OuS9LMyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Directory paths\n",
        "train_dir = \"/content/drive/MyDrive/AIML/Capstone Project/extracted_data/CarImages/Train Images\"\n",
        "test_dir = \"/content/drive/MyDrive/AIML/Capstone Project/extracted_data/CarImages/Test Images\"\n",
        "\n",
        "# Load training dataset\n",
        "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"int\",\n",
        "    class_names=None,\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=32,\n",
        "    image_size=(224, 224),\n",
        "    shuffle=True,\n",
        "    seed=42,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        ")\n",
        "\n",
        "# Load validation dataset\n",
        "validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"int\",\n",
        "    class_names=None,\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=32,\n",
        "    image_size=(224, 224),\n",
        "    shuffle=True,\n",
        "    seed=42,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        ")\n",
        "\n",
        "# Load testing dataset\n",
        "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"int\",\n",
        "    class_names=None,\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=32,\n",
        "    image_size=(224, 224),\n",
        "    shuffle=False,\n",
        ")"
      ],
      "metadata": {
        "id": "jW8jPCh2RR9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Map training and testing images to its annotations."
      ],
      "metadata": {
        "id": "ZA4ag7_oLQCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Directory containing annotations\n",
        "annotations_dir = \"/content/drive/MyDrive/AIML/Capstone Project/extracted_data/Annotations\"\n",
        "\n",
        "# Read training annotations CSV\n",
        "train_annotations_path = os.path.join(annotations_dir, \"Train Annotations.csv\")\n",
        "train_annotations_df = pd.read_csv(train_annotations_path)\n",
        "\n",
        "# Read testing annotations CSV\n",
        "test_annotations_path = os.path.join(annotations_dir, \"Test Annotation.csv\")\n",
        "test_annotations_df = pd.read_csv(test_annotations_path)\n",
        "\n",
        "# Display the structure of the DataFrames\n",
        "print(\"Train Annotations DataFrame:\")\n",
        "print(train_annotations_df.head())\n",
        "\n",
        "print(\"\\nTest Annotations DataFrame:\")\n",
        "print(test_annotations_df.head())\n"
      ],
      "metadata": {
        "id": "AvIOhN8qPVot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "â€£ Step 4: Display images with bounding box."
      ],
      "metadata": {
        "id": "yHADCpYPUhA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print contents of the training directory\n",
        "print(\"Contents of the training directory:\", os.listdir(train_dir))"
      ],
      "metadata": {
        "id": "0StST-OUSEBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the column names in the annotation DataFrame\n",
        "print(\"Annotation DataFrame columns:\", train_annotations_df.columns)"
      ],
      "metadata": {
        "id": "RSu_EkAdT4-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "# Function to display image with bounding boxes\n",
        "def display_image_with_boxes(image_path, annotations_df):\n",
        "    # Load the image\n",
        "    img = image.load_img(image_path)\n",
        "    img_array = image.img_to_array(img)\n",
        "\n",
        "    # Display the image\n",
        "    plt.imshow(img_array.astype(\"uint8\"))\n",
        "\n",
        "    # Plot bounding boxes\n",
        "    for index, row in annotations_df.iterrows():\n",
        "        left, top, width, height = row[\"Unnamed: 2\"], row[\"Unnamed: 3\"], row[\"Unnamed: 4\"], row[\"Bounding Box coordinates\"]\n",
        "        xmin, ymin, xmax, ymax = left, top, left + width, top + height\n",
        "        rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, linewidth=1, edgecolor=\"r\", facecolor=\"none\")\n",
        "        plt.gca().add_patch(rect)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Xs8bcCUVR42o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage for training image with bounding boxes\n",
        "train_example_image_path = os.path.join(train_dir, \"Dodge Dakota Crew Cab 2010\", \"00202.jpg\")  # Replace with an actual image path\n",
        "display_image_with_boxes(train_example_image_path, train_annotations_df)"
      ],
      "metadata": {
        "id": "ds3nqKUcSfWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6kMur7QcUpM2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}